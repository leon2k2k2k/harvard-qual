\documentclass[main.tex]{subfiles}

\begin{document}

\section{Real Analysis}

We review over Banach and Hilbert spaces, measure theory and Lebesgue integration, Fourier theory (series and functions). 

We are following these notes: \url{https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf}

\todo[inline]{Sobolev theory and Baire category theorem (open mapping theorme, closed graph theorem, and uniform boundedness principal}

\subsection{Cheat sheet}
I begin to realize that this is getting way to long, so I am going to have a cheat sheet of all the most important theorems. 

\subsubsection{Basic Real Analysis}
Basic questions in real analysis, like continuity and uniform convergence and stuff:

Uniform convergent implies continuity of the limit:

\begin{theorem}
If $f_n$ uniformly converges to $f$, then if $f_n$ is continuous, then so is $f$.
\end{theorem}

Also derivatives:

\begin{theorem}
If $f_n$ are differentiable with $f_n$ uniformly converges to $f$ and $f' _n$ uniformly converges to $g$, then $g = f'$.
\end{theorem}

We also have the Arzela-Ascoli theorem, which tells us gives us a way to prove a linear operator between Banach spaces is compact.

\begin{theorem}[Arzela-Ascoli Theorem]
Consider a sequence of real-valued continuous functions ${f}_n$ defined on a closed and bounded interval [a, b] of the real line. If this sequence is uniformly bounded and uniformly equicontinuous, then there exists a subsequence ${f}_k$that converges uniformly.
\end{theorem}



\subsubsection{Topology}
A space is compact iff any decreasing sequence of nonempty closed subset has a nonempty intersection.
 
We have the all-important contraction mapping theorem:
\begin{theorem}
If $T: X \rightarrow X$ is a contraction on a complete metric space $X$, that is, $d(Tx, Ty) \leq c d(x,y)$ for some constant $c \leq 1$, then there exists exactly one solution $x$ for the fix point equation $T(x) = x$.
\end{theorem}



\subsubsection{Banach Spaces}
To show that $f_j \rightarrow f$ implies that they are $L^1$ convergent (ofc with additional hypothesis), normally, we can separate $g_j = |f - f_j|$ into two parts, one part is $h_j = g_j \chi_{g_j < M}$ for some $M$. This part always goes to zero as it is dominated by integrable function $M$ (on a finite measure space). Thus we only have to worry about "the large part" $k_j = g_j - h_j = g_j \chi_{g_j \geq M}$, where hopefully the other hypothesis will help us.

\subsubsection{$L^p$ functions}

$L^p$ function on measure space $X$ can be realized as Lebesgue measureable function that has a finite $L^p$ norm modulo thus those integral is zero.

When $X$ is $\mathbb{R}^n$, regularity of Lebesgue measure tells us that compact generated (smooth) continuous functions are dense in $L^p$.

When $X$ is a finite measure space, then by Holder inequality we see that for $1 \leq p \leq q \leq \infty$, then $L^q \subset L^p$, and this map is continuous, that is, the $L^p$ norm can be bounded by the $L^q$ norm. By above, since compact generated continuous functions are dense in both (with their respective topology), this is not a close map.

Thus we have uniform convergence implies uniform a.e. - $L^\infty$ implies $L^p$ implies $L^1$ implies convergence in measure, for finite measure spaces.

\begin{theorem}[Arzela-Ascoli Theorem]
Consider a sequence of real-valued continuous functions ${f}_n$ defined on a closed and bounded interval [a, b] of the real line. If this sequence is uniformly bounded and uniformly equicontinuous, then there exists a subsequence ${f}_k$that converges uniformly.
\end{theorem}




\subsubsection{Fourier series}
Fourier series gives equivalence of Hilbert spaces between $L^(S^1) \cong l^2$, where $l^2 \coloneqq L^2(\mathbb{Z})$. It takes derivative to multiplication by identity on $\mathbb{Z}$, it takes convolution to pointwise multiplication.

The decay of the Fourier series $a_n$ relates to the smoothness of $f$, namely if $f$ is $k$-times derivable, then $a_n$ are $O(n^{-k})$ as $k a_n$ is bounded. 

If $f$ is piecewise smooth, then the Fourier series converges to $f$ at continuous point and is the average of left and right limit at jump points. So if $f$ is smooth (continuous and first derivative continuous), then the Fourier series converge.

We have the Poisson Summation theorem:
\begin{theorem}[Poisson Summation Theorem]
For a large class of functions $f$, we have that 
$$
\sum_{n = - \infty}{\infty} f(2\pi n) = \frac{1}{\sqrt{2 \pi}} \sum_{n = - \infty} {\infty} \hat{f}(n)
$$
\end{theorem}

\subsubsection{Fourier Transform}
If $f$ is compactly generated, then $\hat{f}$ can be extended to a holomorphic function. For example, this implies that if $f$ and $\hat{f}$ is compactly supported, this implies that $f = 0$.


Fourier transform of a Gaussian is another Guassian.

We have the Plancherel theorem, a version of Parseval theorem for continuous spectrum:

\begin{theorem}
If $f$ is both $L^1(\mathbb{R})$ and $L^2(\mathbb{R})$, then $\hat{f}$ is $L^2$ and 
$$
\norm{f}_{L^2} = \norm_{\hat{f}}_{L^2}
$$
In addition, this map extends to a map $L^2 \rightarrow L^2$ that is an isometry. 
\end{theorem}

Thus for any function $f \in L^2(\mathbb{R}$, we can approximate it by $L^1$ functions and the Fourier transform of $f$ is the limit of the Fourier transform of the $L^1$ functions. It is an unitary transformation, that is, it is invertible. With the inverse
\subsubsection{Inequalities}

We have the important Cauchy's inequality:
let $V$ be an inner product space, then 
\begin{theorem}[Cauchy's inequality]
$$(u,v)^2 \leq (u,u) (v,v)$$
with equality iff $u,v$ are linear dependent.
\end{theorem}

We also have Holder's inequality:

\begin{theorem}[Holder's Inequality]
Given $r, p, q$ with $\frac{1}{r} = \frac{1}{p} + \frac{1}{q}$, then 
$$
\norm{fg}_r \leq \norm{f}_{p}  \norm{f}_q
$$

A space is compact iff every family of closed subset having the finite intersection property  has non-empty intersection.

\end{theorem}

We also have the Jensen's inequality:

\begin{theorem}[Jensen's Inequality]
Given a finite measure space $X$ with total measure $\mu(X) = 1$ and a $L^1$ function $f$ on it. Let $\phi$ be a convex function on $\mathbb{R}$ (like $x log x, x^2$), then 
$$
\phi(\int f d\mu) \leq \int (\phi (f)) d\mu
$$
\end{theorem}

In probability, that is, we work with a finite measureable space with total measure 1. Given a random variable $X$ (that is a measureable function), we have the Markov's inequality:

\begin{theorem}[Markov's Inequality]
If $X$ is nonnegative, then 
$$
\mathbb{P}(X > a) < \frac{\mathbb{E}(X)}{a}
$$
\end{theorem}

From this we have the Chebyshev's Inequality:

\begin{theorem}[Chebyshev's Inequality]
For any $X$, we have 
$$
\mathbb{P}[|X - \mathbb{E}[X]| \leq a] \leq \frac{var(X)}{a^2}
$$
\end{theorem}






\subsection{Banach Spaces}
In analysis there is a lot of infinite dimensional spaces, by themselves they are hard to control. Banach spaces are vector spaces complete to a norm and they are easier to control. 

\begin{remark}
Given a Banach space and its norm, we have a notion of convergence. Normally the Banach space comes from the metric completion of an dense subset, which is normally nice (like compact supported smooth (infinitely differentiable). Therefore the space is entirely controlled by these nice function together with the notnio of convergence.
\end{remark}

\subsubsection{Basics}

We first start with the definition of a norm:

\begin{definition}
A norm on a vector space $V$ is a real-valued map $\norm{-} : X \rightarrow \mathbb{R}$, with the condition that 
\begin{enumerate}
    \item $\norm{x + y} \leq \norm{x} + \norm{y}$
    \item $\norm{s x} = |s| \norm{x}$, where $s \in \mathbb{R}$ is a scalar and $|s|$ is its aboslute value.
    \item If $\norm{x} = 0$ then $x = 0$.
\end{enumerate}

A norm give a metric on $V$ by $d(x, y) = \norm{x -y}$
\end{definition}

\begin{example}
Example of normed spaces are $L^p$ spaces. Inner product $(-,-)$ also gives norms by $\norm{v} \coloneqq \sqrt{(v,v)}$. 
\end{example}


\begin{definition}
A Banach space $V$ is a normed vector space that is complete with respect to the metric derived from the norm. A Banach space is separable if there exists a countable dense subset of it.
\end{definition}

The dual of a separable Banach space is not necessarily separable, however, 

\begin{proposition}
If $X^*$ is separable, then so is $X$.
\end{proposition}


A closed subspace of a Banach space is always Banach. However, there might be open dense subspaces (and the Banach space is the completion).

For linear maps between normed spaces, there is a notion of bounded linear maps:

\begin{definition}
$T: X \rightarrow Y$ is bounded if exists $M \geq 0$ such that 
$$
\norm{Tx} \leq M \norm{x}
$$
for all $x \in X$. The operator norm of $T$ is 
$$
\norm{T} \coloneqq inf \{ M | \norm{Tx} \leq M \norm{x}
$$
\end{definition}


Many linear maps are not bounded, such as taking derivatives, such as taking derivative on smooth functions. 

In fact, a linear map is bounded iff it is continuous:

\begin{theorem}
A linear map between normed spaces is bounded iff it is continuous.
\end{theorem}

Moreover, it is suffice to suffice to check on a dense subspace. This is called the BLT theorem:

\begin{theorem}[Bounded Linear Transformation]
Let $X$ be a normed linear space and $Y$ a Banach space. If $M$ is a dense lienar subspace of $X$ and $T: M \rightarrow Y$ is a bounded linear map, then there is a unique extension to $\hat{T}: X \rightarrow Y$ with the same operator norm.
\end{theorem}

We only care about norm up to topology:

\begin{definition}
Two norms on $X$ are equivalent if there exists constants $c , C > 0$ such that 
$$
c\norm{x}_1 \leq \norm{x}_2 \leq C\norm{x}_1
$$
\end{definition}

\begin{theorem}
Two norms induces the same topology iff they are equivalent
\end{theorem}

Here's the open mapping theorem:

\begin{theorem}[Open Mapping Theorem]
Suppose $T: X \rightarrow Y$ is one-to-one, onto bounded linear map between Banach Spaces, then $T^{-1} : Y \rightarrow X$ is bounded.
\end{theorem}


Note that every finite dimensional normed vector spaces are Banach spaces and all the norms are equivalent, thus their topology are the same.

Given a Banach space $X$ and a close subspace $M$, then we have the quotient $X/M$ with the qoutient norm:
$$
\norm{[x]} = inf_{v \in M} \{\norm_{v + x} \}
$$
This in fact makes $X/M$ a Banach space and the projection map $X \rightarrow X/M$ continuous.

\subsubsection{Bounded Operators}
Now we study bounded operators. 

First of all, bounded operator composes: for $T: X \rightarrow Y$ and $S: Y \rightarrow Z$, the composition $ST$ is bounded with operator norm $\norm{ST} \leq \norm{S}\norm{T}$. 
The space of bounded mao=ps $B(X,Y)$ is a Banach space if $Y$ is one:
\begin{theorem}
If $X$ is a normed linear space and $Y$ is a Banach space, then $B(X,Y)$ is a Banach space with repsect to the operator norm.
\end{theorem}

An important class of bounded operators are compact ones:

\begin{definition}
A linear operator $T: X \rightarrow Y$ is compact if $T(B)$ is precompact subset of $Y$ for every bounded subset $B$ of $X$.
\end{definition}

Equivalently, $T$ is compact iff every bounded sequence $(x_n)$ in $X$ has a subsequence whose image converges in $Y$. A subset is precompact if its closure is compact. 

To show a map is compact, we normally use Ascola theorem. To show it is not, we normally find functions with disjoint support that are bounded, this work really well when we are trying to show that an inclusion Banach spaces generated by diferent norms is not compact.

\begin{proposition}
$K(X,Y)$ the space of compact linear operators is a close linear subspace. It is also a two sided idea, that is, composition of a compact operator and a bounded operator is compact.
\end{proposition}

Compact operators on infinite dimensional spaces behave like operators on finite-dimensional spaces. On Hilbert spaces they are uniform limit of operators with finite rank.

There is also different notions of convergence. Converging in the operator norm is uniform convergence. We also have another notion, called strong convergence.

\begin{definition}
A sequence $T_n$ in $B(X,Y)$ converges strongly if it converges pointwise to $T$.
\end{definition}
Uniform implies strongly but not reverse (the same argument as uniform convergence of function and point-wise convergence).

We can define the exponential of a bounded linear function:
$$
e^A \coloneqq I + A + \frac{1}{2!}A^2 + ...
$$
It has norm less than $e^{\norm{A}}$.

If $A$ and $B$ commute, we have $e^A e^B = e^{A + B}$. This gives rise to a flow, which is a one-parameter uniformly continuous group. 


\subsubsection{Dual Spaces}
The topological dual of $X$ is the space of continuous (bounded) functionals.

For Hilbert spaces, the Riesz representaiton theorem tells us that the vector space can be identified with the origin space. Not true for general Banach spaces.

Hans-Banach says that we can extend bounded linear operator defined on a subspace:

\begin{theorem}[Hans-Banach]
If $Y$ is a linear subspace of normed space $X$ and $\phi: Y \rightarrow \mathbb{R}$ is a bounded linear functional on $Y$ with norm $M$, then exists an extension $\phi': X \rightarrow \mathbb{R}$ that restricts to $\phi$ on $Y$ and has the same norm.
\end{theorem}

This gives us a notion of weak convergence:
\begin{definition}
A sequence $x_n$ in Banach space $X$ converges weakly to $x$ if 
$$
\phi_(x_n) \rightarrow \phi(x)
$$
for every bounded linear functional $\phi$ in $X^*$.
\end{definition}

We also have weak $*$ convergence for the dual space $X^*$:
\begin{definition}
A sequence $\phi_n$ in the dual Banach space $X^*$ weakly $*$ converge to $\phi$ if
$$
\phi_n(x) \rightarrow \phi(x)
$$
for every $x \in X$.
\end{definition}
It generates the weakest topology such that the pairing $X \times X^* \rightarrow \mathbb{R}$ is continuous.



\subsection{Hilbert Spaces}
Banach spaces are pretty nice spaces, however, they are still not very intuitive. Hilbert spaces are Banach spaces with the norm coming from an inner product. They behave closer to finite dimensional vector spaces. And as we will see, in some sense, there are "only one" Hilbert space of a given (orthonormal basis) size.


\subsubsection{Basics}
We will define it or complex linear spaces, for real vector spaces just omit the complex conjugates:

\begin{definition}
An inner product on $V$ is a map 
$$
(-, -): X \times X \rightarrow \mathbb{C}
$$
such that:
\begin{enumerate}
    \item It is linear in the second factor.
    \item $(y,x) = \overline{(x,y)}$ (Hermitian symmetric)
    \item $(x,x) \geq 0$
    \item $(x,x)$ = 0 iff $x = 0$
\end{enumerate}
\end{definition}

From an inner product, we can define a norm by 
$$
\norm{x} = \sqrt{(x,x)}
$$

\begin{definition}
A Hilbert space is a complete inner product space.
\end{definition}

\begin{example}
The standard inner product on $\mathbb{C}^n$ makes $\mathbb{C}^n$ a Hilbert Space. $L^2(X)$ is an inner product space with inner product
$$
(f,g) = \int_a ^b \overline{f}g d\mu
$$
The other $L^p$ spaces are not Hilbert spaces.

Let $C^k([a,b])$ the space of functions with $k$ continuous derivatives. We have an inner product:
$$
(f,g) = \sum_{j=0} ^k \int_a ^b \overline{f^{(j)}(x)} g^{(j)}(x) dx,
$$
$f^{(j)}$ is the $j$-th derivative. Then the completion is called the Sobolev space $H^k((a,b)) = W^{k,2}((a,b))$.
\end{example}

From a norm, it is a condition if it comes from an inner product or not:

\begin{theorem}
A norm comes from an inner product iff 
$$
\norm{x + y}^2 + \norm{x - y}^2 = 2\norm{x}^2 + 2\norm{y}^2
$$
\end{theorem}

Lastly, the inner product $X \times X \rightarrow \mathbb{C}$ is a continuous map.

In Hilbert spaces, we have the notion of orthogonality. The orthogonal complement of a subset if a closed linear subspace.

\begin{theorem}[Projection]
For a closed subspace $M \subset X$, for any point $x$, there is a unique closest point. It is the point $y$ where $(x - y)$ is orthogonal to $M$. This also means that $\mathcal{H} = M \oplus M^\perp$.

\subsubsection{Orthonormal Bases}
Orthonormal basis are nice. Two hilbert spaces with orthonormal bases have the same cardinality are isomorphic. A separable Hilbert space has a finite or countably infinite orthonormal basis. For infinite dimensional Hilbert spaces, the notion of orthonormal basis is about infinite sums, not finite sums. There is a notion of absolute convergence and sum over an (possibly uncountable) elements. 
However, we have the Bassel's inequality:

\begin{theorem}[Bassel's inequality]
Given an orthonormal set $u_\alpha$ and $x \in H$, then \begin{enumerate}
    \item $\sum_{\alpha} |(u_\alpha, x)|^2 \leq \norm{x}^2$
    \item $x_U = \sum_\alpha (u_\alpha, x)u_\alpha$ is a convergent sum
    \item $x - x_U \in U^\perp$
\end{enumerate}
\end{theorem}

Given a subset $U$ of $H$, there is a notion of the closed linear span, being the infinite sums that converges unconditionally. It is the smallest closed linear subspace that contains $U$.

Now we can define orthonormal basis (this is really a theorem):

\begin{definition}
A set of orthonormal element $u_\alpha$ is complete (an orthonormal basis) if one of the following equivalent condition is satisfied:
\begin{enumerate}
    \item $(u_\alpha, x) = 0$ for all $\alpha$ implies $x = 0$
    \item $x = \sum_\alpha (u_\alpha, x) u_\alpha$
    \item $\norm{x}^2 = \sum_\alpha |(u_\alpha, x)|^2$
    \item the closed linear span $[U] = H$
    \item $U$ is a maximal orthonormal set.
\end{enumerate}
\end{definition}

\begin{remark}
the first condition is the easiest to verify, and second is used most often. 
\end{remark}

We have the Parseval's identity:
\begin{theorem}[Parseval's Identity]
Suppose $u_\alpha$ is an orthonormal basis of $H$, if $x = \sum_\alpha a_\alpha u_\alpha$, $y = \sum_\alpha b_\alpha u_\alpha$, then 
$$
(x,y) = \sum_\alpha \overline{a_\alpha} b_{\alpha}
$$
\end{theorem}

A corollary of this is that any Hilbert space $H$ with orthonormal basis $U$  is isomorphic to $l^2(U)$. By Zorn's lemma, any Hilbert space has an orthonormal basis.

The Gram-Schmidt orthonormalization procedure constructs orthonormal basis from any countable linearly independent set whose linear span is dense in H.


Examples of orthonormal basis is the standard basis of $\mathbb{C}^n$, the delta basis of $l^2(\mathbb{Z})$, 
$$
e_n(x) = \frac{1}{\sqrt{2\pi}} e^{inx}
$$
is an orthonormal basis of $L^2(T)$. 


\end{theorem}

\subsubsection{Bounded Operator on Hilbert Spaces}
Bounded operators behave better on Hilbert spaces, because we have the adjoint of a bounded linear map. 

First we have the theory of Orthogonal projections:

\begin{definition}
An orthogonal projection on Hilbert space $H$ is $P : H \rightarrow H$ such that $P^2= P$ and $<Px, y> = <x, Py>$. 
\end{definition}
A projection is bounded with norm $1$. They corresponds to closed subspaces (being the image). The kernel is the orthogonal complement.

\begin{example}
Take $H = L^2(T)$, $u = \frac{1}{\sqrt{2 \pi}}$ constant function with norm one. Then the projection $P_u$ takes $f$ to its mean $<f> = \frac{1}{2 \pi} \int_0 ^{2\pi} f(x) dx$. The orthogonal decomposition 

$$
f(x) = <f> + f'(x)
$$
with $f'(x)$ the fluctuation around the mean (it is not the derivative).
\end{example}
From this we can easily deduce that all bounded linear functionals are of the form $\phi_y(x) \coloneqq <y, x>$ for some $y \in H$:

\begin{theorem}[Riesz representation]
For any $\phi$ bounded linear functional on $H$, there exists an unique vector $y$ such that 
$$
\phi(x) = <y,x>
$$
\end{theorem}
The map $<-, x>: H \rightarrow H^*$ is an isometry isomorphism. For compelx Hilbert spaces, $HJ$ is antilinear. So Hilbert spaces are self-dual, $H$ and $H^*$ are isomorphic as Banach spaces and anti-isomorphic as Hilbert spaces. This is a special aspect of Hilbert spces.

\subsubection{adjoint of an operator}
From Riesz representation theorem we also get the adjoint of a bounded operator. For $A \in B(H)$, the adjoint statisfies the property that 
$$
<x, Ay> = <A^* x, y>
$$
This exists and is unique by Riesz representation theorem. The adjoint of a matrix (linear map $\mathbb{R}^n$ to $\mathbb{R}^m$ with standard inner prodcuts) is its transpose matrix. For complex $\mathbb{C}^n$, it is the Hermitian conjuagate.

Adjoint is important for studying the solvaability of $Ax = y$. For any $z$ with $A^* z = 0$, then $z$ is perpendicular to imgaes of $A$. 

\begin{theorem}
For $A: H \rightarrow H'$, we have 
$$
\overline{ran A} = (ker A^*)^\perp, ker A = (ran A^*)^\perp
$$
\end{theorem}
Given $A: H \rightarrow H'$ bounded, then we have direct sum 
$$
H' = \overline{ran A} \oplus ker A^*
$$
Thus if $A$ has closed image, it is even simpler. Recall that if we have $c \norm{x} \leq \norm{Ax}$ then the map is injective and has closed range. 

A bounded operator with close range can have no kernel and finite cokernel $(ker A^*)$, or vice versa. Think about the right and left shift operator on $l^2$. 

\begin{definition}
A bounded linear operator $A$ on Hilbert space is Fredholm if
\begin{enumerate}
    \item $ran A$ is closed
    \item $ker A$ and $ker A^*$ are finite-dimensional
\end{enumerate}
\end{definition}
It has an invariant, called index:
$$
ind A \coloneqq dim ker A = dim\ ker \ A^*
$$

If $A$ is Fredholm and $k$ is compact, then $A + K$ is Fredholm and $ind(A + K) = ind A$. So the index is unchanged by compact perturbations. It is actually an topological invariant (we have the space of Fredholm operator, which is open in the Banach space of bounded operators).

\subsubsection{Self-adjoint and unitary operators}
The two most impiortant classes are self-adjoint and unitary. We fix $H$ and $A: H \rightarrow H$ bounded.

\begin{definition}
$A$ is self-adjoint if $A^* = A$.
\end{definition}

\begin{example}
For $\mathbb{R}^n$, $A$ is self-adjoint iff it is symmetric. On $\mathbb{C}^n$ it is Hermitian.
\end{example}

Given $A$, we have sesquilinear for $a: H \times H \rightarrow \mathbb{C}$ with $a(x,y) \coloneq <x, Ay>$. If $A$ is self-adjoint, then it is symmetric:
$$
a(x,y) = \overline{a(y,x)}
$$
so $q(x) = a(x,x) = <x, Ax>$ is real-valued. $A$ is nonnegative if it is self-adjoint and $q \geq 0$. It is positive definite if it is positive and $q(x) > 0$ for non-zero $x$. It defines an inner product on $H$. If $<x, Ax> \geq c \norm{x}^2 = c <x,x>$, it is called bounded from below. The norm associated to $(-,-)$ is equivalent to that of $<-,->$.

The norm of a self-adjoint operator is $\norm{A} = sup_{\norm{x} = 1}|<x, Ax>|$
Given $A$ bounded, then $A^* A$ is self-adjoint, and $\norm{A^* A} = \norm{A}^2$
This also shows that $\norm{A} = \norm{A^*}$.

We also have unitary, which is the notion of isomorphism of Hilbert spaces:

\begin{definition}
A linear map $U: H \rightarrow H$ is unitary if it is invertible and 
$$
<Ux, Uy> = <x,y>
$$
A map $U$ is unitary iff $U^*U = Id$ and $U U^* = Id$.
\end{definition}
For fintie dimensional real, $A$ is orthogonal if $Q^T = Q^{-1}$, for complex it is $U^* = U^{-1}$

Given $A$ a bounded self-adjoint operator, then $e^{iA}$ is unitary. A bounded operator $S$ is sekw-adjoint if $S^* = -S$. $S = iA$ for A self-adjoint. Then Lie algebra of the Lie group of Unitary operators is the space of bounded, skew-adjoint operators with the commutator Lie bracket. 

For example, the translation operator is unitary on $L^2$. A good class of operator is normal:

\begin{definition}
$T: H \rightarrow H$ is normal if $TT^ * = T^* T$, aka $T$ commutes with adjoint. They have a nice spectral theory.
\end{definition}

\subsubsection{Weak convergence in Hilbert Space}
A sequence $(x_n$ in $H$ converges weakly to $x \in H$ if
$$
lim_{n \rightarrow \infty}<x_n, y> = <x,y>
$$
Strong convergence implies weak convergence but not the other way:

\begin{example}
Let $H + l^2(N)$ and $e_n$ is the $n$-th standard basis. Then $<e_n, y> = y_n \rightarrow 0$ as the coefficients of $y$ converge to $0$ (as $y$ is $L^2$). Thus $e_n$ converges weakly to $0$ but not strongly.
\end{example}

We have a criterion for weakly convergent sequence:

\begin{theorem}
Given $(x_n)$ sequence in $H$ and $D$ a dense subset, then $(x_n)$ converges weakly to $x$ iff 
\begin{enumerate}
    \item $\norm{x_n} < M$ for some $M$ independent of $n$
    \item $<x_n, y) \rightarrow <x,y>$ for all $y \in D$.
\end{enumerate}
\end{theorem}

Thus given $e_\alpha$ orthonormal basis, then $x_n$ converges weakly to $x$ iff it is bounded and its coordinates converge for every $\alpha$.

We like weak convergence because it is easier for sets to be compact in the weak topology than the strong one. In fact, a set is weakly precompact iff it is bounded. A version of Heine-Borel theorem for infinite-dimensional spaces. 

\begin{theorem}[Banach-Alaoglu]
The closed unit ball of a hilbert space is weakly compact, that is, compact in the weak topology.
\end{theorem}

\subsubsection{Spectrum of Bounded Linear Operator}
In many cases, we would like to diagonalize a linear map, that is, find its eigenvalues and eigenvectors. This is called the spectral theory. There are continuous and point spectrums. Compact opeartors are nice.

In the finite dimensional case we have that 
\begin{theorem}
an $n \times n$ matrix $A$ is normal iff there is an orthonormal basis consists of eigenvectors of $A$.
\end{theorem}

In the infinite dimensional case, a bounded linear operator might not have eigenvalues at all. We define the spectrum of $A$ as follows
\begin{definition}
The resolvent set of $A$ is the set of complex numbers $\lambda$ such that $A - \lambda I: H \rightarrow H$ is an iso. The spectrum of $A$, $\sigma(A)$ is the complement.
\end{definition}

The points in the spectrum can be divided more:
\begin{definition}
\begin{enumerate}
    \item The point spectrum of $A$ are those $\lambda$ with $A - \lambda I$ not one-to-one, $\lambda$ is called an eigenvalue.
    \item continuous pectrum of $A$ are $\lambda$ such that $A - \lambda I$ is one-to-one but not onto, and $ran(A - \lambda I)$ is dense.
    \item residual is one-to-one, not onto, and not dense.
\end{enumerate}
\end{definition}
At resolvent points, we have inverse $R_\lambda$. This is defined on the subset of $|mathbb{C}$ of resolvent point of $A$. An operator-valued function $F: \Omega \rightarrow B(H)$, $\Omega$ open subset of $\mathbb{C}$, is called analyitc at $z_0 \in Omega$ if it has a convergent power series with respect to the operator norm on $B(H)$. It is called analytic or holomophic if it is analytic at every point in $\Omega$.

If $A$ is bounded, then $\rho(A)$ the resolvent set if an open set, and the resulvent is an operator-valued analytic function of $\lambda$. .This is because we can write Neumann series. Thus the spectrum is closed. Its radius is the smallest disk centered at zero containing $\sigma(A)$.

If $A$ is bounded, then $r(A) = lim_{n \rightarrow \infty} \norm{A^n}^{1/n}$, if $A$ is self-adjoint, then $r(A) = \norm{A}$.

This implies that the spectrum of a bounded operator is not empty.

\subsubsection{Spectral theorem for self-adjoint operators}
For (bounded) self-adjoint operators, the eigenvalues are real and eigenvectors are orthogonal.
Thus we are looking at invariant subspaces. This implies that the spectrum of $A$ is contained in the interval $-[\norm{A}, \norm{A}]$. In fact, the residual spectrum of a bounded, self-adjoint operator is empty.


The bounded, compact, self-adjoint ones behave much more like finite dimensional ones. 

A nonzero eigenvalue of a compat, self-adjoint operator has finite multiplicity. A countable infinite set of nonzero eigenvaluehas zero asn an accumulation point, and no others.

The spectral theorem that says that any compact operator looks like countable sums of projections to finite -dimensional eigenspaces multiply by $\lambda_k$:

\begin{theorem}
If $A : H \rightarrow H$ bounded, compact, self-adjoint. Then there exists orthonormal basis of $H$ consists of eigenvectors of $A$. The nonzero eigenvectors forms a countable infinite set $\lambda_k$ with $A = \sum_k \lambda_k P_k$ where $P_k$ is orthogonal projection to the finite-dimensional eigenspace with eigenvalues $\lambda_k$. It converges in the operator norm.
\end{theorem}

\subsubsection{Compact operators}
to check an operator is comapct, best to use Ascoli or this:

\begin{theroem}
A bounded linear operator on Hilbert space is comact iff it map weakly convergent sequences to strongly convergent sequences.
\end{theroem}

\subsection{Linear Differential Operators and Green's functions}

Linear differential operators are important. However, they are often unbounded. There are two approaches to study them, one is use weak topology where differential operators are continuous. This give rise to distribution theory. The other is to consider those that are defined on a dense subspace of a Hilbert or Banach space. 

The inverse of a linear differential operator is an integral operator, the kernel is the Green's function. Use bounded inverse to study the unbounded differential operator. 

\subsubsection{Unbounded operators}
They are normally defined on a dense subspace $A: D(A) \subset H \rightarrow H$. An extension is an extention of domains. Domain decodes smoothness and boundary conditions.

The adjoint of an unbounded operator $A$ is operator $A^*$, defined on the largest subspace such that 
$$
<Ax,y> = <x, A^* y>
$$
holds for all $x \in D(A)$. $D(A^*)$ is the space of all $y$ such that $\phi_y \coloneqq <y, A -> $is bounded. It is possible that $D(A^*)$ is not dense so we can't define $A^**$.

\begin{remark}
Note this $A^*$ depends on the domain, I think.
\end{remark}

The adjoint of a differential operator is another differential operator, by integration by parts. The domain $D(A)$ defines boundary conditions for $A$ and $D(A^*)$ defines adjoint boundary conditions, they are there to make sure the boundary terms from integration by parts vanish. An important class is self-adjoint ones, $A = A^*$, this also means the domains are equal. This is a self-adjointness of the boundary conditions.

\begin{defintion}
An unbounded operator is self-adjoint if $A  = A^*$. An unbounded operator $A$ is symmetric if $A^*$ is an extension of $A$.
\end{defintion}

Differential operators also have the notion of closedness:

\begin{definition}
An operator $A$ is closed if for every sequence $(x_n)$ in $D(A)$ such that $x_n \rightarrow x$ and $A x_n \rightarrow y$, then $x \in D(A)$ and $Ax = y$.
\end{definition}

This means the graph of $A$, $\Gamma(A) \subset H \times H$ is a closed subset.

An operator is closable if every $(x_n)$ in $D(A)$ such that $x_n \rightarrow 0$ and $A x_n \rightarrow y$, then $y = 0$. Then we can define the closure of a closable operator. Its graph is the closure of the graph of $A$. Every symmetric operator is closable. Symmetric operator $A$ is essentially self-adjoint if its closure is self-adjoint.

If $A$ is one-to-one and onto, then we have $A^{-1}: H \rightarrow H$. Its range is $D(A)$. If $A$ is closed, then by the closed graph theorem, $A^{-1}$ is bounded. Then $(A^*)^{-1} = (A^{-1})^*$. We also have resolvent set, spectrum, and resolvent operator. By the closed graph theorem, if $A$ is closed, $R_\lambda$ is bounded whenever $A - \lambda I$ is one-to-one and onto. It may have an empty spectrum.

\subsubsection{Adjoint of a differential operator}
A linear ordinary differential operator of order $n$ is a linear mpa $A$ acts on $n$-times continuously differentiable function $u$ by 
$$
Au = \sum_{j=0} ^n a_j u^{(j)}
$$

We want to study boundary value problems (BVP)
$$
Au = f, Bu = 0
$$
$Bu = 0$ is a set of linear boundary conditions.

For example, consider functions on $[0,1]$, with 
$$
Au = a u'' + b u' + cu,
$$
for $a, b, c$ sufficiently smooth. $a(x) > 0$. For second-order, we expect to have two boundary conditions to get unique (sometimes not enough), we also want overdetermined or underdetermined. 
Examples are 
\begin{enumerate}
    \item $u(0) = 0, u(1) = 0$ Dirichlet
    \item $u'(0) =0, u'(1) = 0$ Neumann
    \item $u(0) = u(1), u'(0) = u'(1)$ periodic
    \item $u(0) = u'(0) = 0$ Intial
    \item $u(1) = u'(1) = 0$ final
\end{enumerate}

Nonhomogeneous boundary conditions are affine space for the homogeneous ones. 

We first start by formulating the adjoint boundary value problem. We have $A^* v = (\overline(a)v)'' - (\overline(b)v)' + \overline{c}v$, then $<v, Au> - <A^*v, u>$ is some boundary term. It is called the formal adjoint. The formal adjoint of $D = \frac{d}{dx}$ is $-D$, thus $(iD)^* = iD$ and $(D^2)^* = D^2$. 

The dual boundary condition for $B$ is the requirement for the boundary term to vanish. So $B^*v = 0$ iff $<v, Au> = <A^* v, u> $ for all $u \in C^2([0,1])$ with $Bu = 0$.

\begin{example}
The dual for Dirichlet condition for $A = D^2$ is Dirichlet. So the Dirichlet boundary value problem for $D^2$ is self-adjoint. Same for Neumann, mixed, and periodic are also self-adjoint. However, the adjoint for initial is final. The adjoint of no boundary condition is $v(0) = v'(0) = v(1) = v'(1) = 0$, so adjoint of undertermined BVP is overdetermined. 
\end{example}

\subsubsection{Green's functions}
Consider the Dirichlet boundary value problem for $A$:
$$
Au = f, u(0) = u(1) = 0
$$
We are looking for solution 
$$
u(x) = \int_0 ^1 g(x,y) f(y) dy
$$
This is called the Green's function. 

We regard $A : D(A) \rightarrow C([0,1])$ as an operator, with 
$$
Gf(x) = \int_0 ^1 g(x,y)f(y) dy
$$
as the inverse.

\begin{remark}
So $G$ is one-to-one but not onto, its image is a dense subset. Those are symptons of these inverse operators to unbounded operators.
\end{remark}

We will use delta function $\delta$ as heuristic (it is really a distribution). It is the derivative of the Heaviside step function.

The Green's function $g(x,y)$ is the solution to the BVP:
$$
Ag(x,y) = \delta(x-y), g(0,y) = g(1,y) = 0
$$
$A$ is differential operator with respect to $x$. As a function of $x$, we want $g(x,y)$ to satisfies the homogeneous ODE when $x \neq y$, and we want it to jump at $x = y$. 

We can construct the Greens' function: think about the homogenous ODE 
$$
au'' + bu' + c = 0
$$
has two -dimensional space of solutions. We can find basis $u_1, u_2$ determined by initial conditions $u(0) = 1, u'(0) = 0$ and $u(0) = 0, u'(0) = 1$. 

Note that in nice situations, we have $g^*(x,y)$ the Green's function for the adjoint operator, being $g^*(x,y) = \overline{g(y,x)}$

\subsubsection{Weak derivatives}
Classical differential operators that act on continuously differentiable functions lack desirable properties such as closed or self-adjoint. To get them, we extend the domains of classical differentiation to include functions whose weak derivatives belong in $L^2$. Weak deriavtieves are defined using integration against test functions. They can be approximated by smooth functions. The space of functions with $k$ square-integrable derivates is the Sobolev $H^k$. They give domains of simple self-adjoint orindary differential operators.

A test function $\phi: \mathbb{R} \rightarrow \mathbb{C}$ is a test function if it has compact support and continuous derivatives of all order, call the space $C^\infty _c(\mathbb{R})$. They are dense in $L^2$. We can approximate an $L^2$ function by its convolution with a smooth approximate idenitty, this is called mollification.

Given $\phi$ a test function with support $[-1, 1]$ and 
$$
\int \phi(x) dx = 1
$$
example is $\phi(x) = e^{\frac{-1}{(1-x^2}}$. 
Then $\phi_\epsilon(x) = \frac{1}{\epsilon} \phi(\frac{x}{\epsilon})$ is an approximate identity (for convolution, so delta at $0$) as $\epsilon \rightarrow 0$.  Then the mollification $u_\epsilon = \phi_\epsilon * u$, that is 
$$
u_\epsilon(x) = \int_\mathbb{R} \phi_\epsilon(x-y) u(y) dy
$$
it is in $C^\infty(\mathbb{R}$ as 
$$
u^{(k)} _\epsilon (x) = \int \phi^{(k)} _\epsilon (x-y) u(y) dy
$$
It converges to $u$ in $L^2$.

We can use integration by parts to define weak derivative:

\begin{theorem}
A function $u \in L^2$ has a weak derivative $v \in L^2$ if 
$$
<v, \phi> = -<u, \phi'> 
$$ for all $\phi \in C^\infty _c (\mathbb{R}$. 
The Sobolev space $H^k(\mathbb{R})$ consists of functions with $k$ square-integrable weak derivatives, equipped with inner product
$$
<u,v> = \int_\mathbb{R} \{\overline{u}v + \overline{u}' v' + \overline{u}^{(k)} v^{(k)} \} dx
$$
\end{theorem}

Differential operator on weak derivative is now a closed operator.

Closedness of $D$ implies that $H^k(\mathbb{R})$ is complete thus a Hilbert space.

Another way to define $L^2$ derivatives is the $L^2$ limit of smooth derivative. We say $u' = v$ if there is a sequence of smooth function $u_n$ with $u_n \rightarrow u$ and $u' _n \rightarrow v$. They are equivalent as any $H^k$ functions can be approximated in the $H^k$ norm by a test function. Aka $C^\infty _c (\mathbb{R})$ is smooth.

We have that 
$$
\int u v' dx = - \int u' v dx
$$
for $u, v \in H^1(\mathbb{R})$

Now we can have self-adjoint operators: $A = iD: H^1(\mathbb{R}) \rightarrow L^2(\mathbb{R})$ is self-adjoint. Namely, the domain $D(A^*) = H^1(\mathbb{R})$

We have Sobolev embedding theorem, which says that the elements of the Sobolev spaces are actually continuous (have a continuous representative), and differentiable etc, for nice enough $k$.

\subsection{Distribution}
A distribution is a continuous linear functional on test functions. They are an extension of functions. Delta function are distributions. Every distributions are differentiable, and differentiation is a continuous operation on spaces of distributions. Fourier tranform work well with tempered distributions. One problem is that there is no product of distributions extending pointwise product of functions (they are homological, not cohomological). 

\subsubsection{Schwartz space}
It is the space test functions that consists of smooth, rapidly decreasing functions.

\begin{definition}
A function $\phi \in C^\infty(\mathbb{R}^n)$ is in the Schwartz space $S$ if all of its derivative decay faster than any polynomial as $|x| \rightarrow \infty$.
\end{definition}

Example is a polynomial $q(x)$ times $e^{-c |x - x_0|^2}$. 

We want convergence and topology on $S$. The appropriate topology is coming from a countable family of seminorms. Given a family of seminorms, we have a topology on $X$ such that a sequence $(x_n)$ converges to $x$ iff $p_\alpha(x - x_n) \rightarrow 0$ for every norm $p_\alpha$.

For countably infinite seminorms, then the topology is metrizable. A metrizable, locally convex space that is complete is called a Frechet space.

Schwartz space $S$ with the countable family of seminorms $p_{\alpha, \beta}: \phi \mapsto sup_{x \in \mathbb{R}^n} |x^\alpha \partial^\beta \phi(x)|$, where $\alpha, \beta \in \mathbb{Z}^m _+$ are in multi-indices.

\begin{remark}
The basic idea here is that taking derivative goes from $C^k \rightarrow C^{k-1}$. So to keep it at the same $k$ we need $k = \infty$, where we don't have a good notion of norm. So we need this family of norms.
\end{remark}

\begin{proposition}
Taking partial differentiation $\partial^\alpha: S \rightarrow S$ is a continuous linear operator on $S$.
\end{proposition}

\subsubsection{Tempered Distributions}
The topological dual of $S$, denoted as $S^*$, is the space of continuous linear functionals on $S$> Elements are called tempered distributions. 

$T$ is continuous iff 
$$
|T (\phi)| \leq \sum_{|\alpha|, |\beta| \leq d} c_{\alpha, \beta} \norm{\phi}_{\alpha, \beta}
$$

\begin{example}
The fundamental example is the delta function: 
$$
\delta(\phi) = \phi(0)
$$
\end{example}

Given $f$ Lebesgue measureable with 
$$
|f(x)| \leq g(x) (1 + |x|^2)^{d/2}
$$
a.e. for some $d \geq 0$, $g$ nonnegative, integrable function $g$. Then 
$$
T_f(\phi) = \int f(x) \phi(x) dx
$$
defines a tempered distribution. We can recover $f$ up to pointwise-a.e. by $T_f$ by convolving with approximate identity like Gaussian.

Distributions that are integration against a function $f$ is called a regular distributions. Those that not of this form is called singular. Tempered distributions is generalization of locally integrable functions with polynomial growth. 

Tempered distributions is a subspace of $D^*$ the space of distributions. It is the continuous linear functionals on $D$ the space of smooth, compactly generated test functions. The distributions in $D^*$ can grow faster than polynomial at infinity, and its Fourier transform not beling in $D^*$. For $S^*$, its Fourier transform stays in $S^*$.

\subsubsection{Operations on distributions}
Given $f$ such that it and all its derivative have polynomial growth, then $f\phi \in S$ for $\phi \in S$. Thus it also acts on $S^*$ continuously.

$f \delta = f(0) \delta$. 

We can also take derivatives:

\begin{definition}
$\partial^\alpha T$ is defined as 
$$
<\partial^\alpha T, \phi> = (-1)^{|\alpha|}<T, \partial^\alpha \phi>
$$
\end{definition}
Taking derivative is continuous and every tempered distributions has a derivative. It is the minimal extension of the space of functions of polynomial growth that is closed under differentiation:

\begin{theorem}
For every $T \in S^*$, exists $f$ continuous of polynomial growth and $\alpha$ such that $T = \partial^\alpha f$.
\end{theorem}

If $_f$ is a regular distribution whose distributional derivative is also regular $T_g$, then $g$ is the weak derivative of $f$, $g = \partial^\alpha f$. Weak $L^2$ derivative is a version of this.

Let $f(x)$ be $0$ for $x \leq 0$ and $x$ for $x \geq 0$. It has no derivative. But it has a weak derivative, which is the step function $H$. The distributional derivative of $H$ is the delta function. So $H$ is not weakly differentiable.

Given a continuous linear transform $K: S \rightarrow S$, ofc we get $K'$ the transpose. For example the translation operator gives a translation on distribution.

Convolution of test function exists. We can also convolve a test function with a distribution:
$$
(\phi * T)(x) = <T, R\tau_x \phi>
$$
where $R$ is reflection and $\tau_x$ is shift by $x$. It has at most polynomial growth.

Convolving with $delta$ gives identity: $(\phi * \delta) = \phi$.

There is a notion of weak convergence. It is the convergence corresponding to the weakest topology such taht all functional $T \mapsto <T, \phi> $ is continuous. It is called the weak $*$ topology, generated by the semi-norms $p_\phi$:
$$
p_\phi(T) = |<T, \phi>|
$$

\begin{theorem}
Schwartz space is dense in the space of tempered distributions
\end{theorem}
$S$ is also dense in $C_0$ with respect to uniform convergence, and in $L^p$ for all $1 \leq p < \infty$.


%%%%%%%%%%


\subsection{Fourier Series}
Fourier Series takes a function on $T = S^1$ (periodic function) to a series. It turns out the behavior of the series says a lot about the function.

\subsubsection{Basics}
Let $C(T)$ be the space of continuous functions, and $L^2(T)$ the completion to the $L^2$-norm.
It can be concretely realized as equivalence class of Lebesgue measureable, square integrable functions from $T$ to $\mathbb{C}$. It is a Hilbert space. 

The Fourier basis elements are functions
$$
e_n(x) = \frac{1}{\sqrt{2 \pi}} e^{inx}
$$

\begin{theorem}
They form an orthonormal basis of $L^2(T)$, thus $L^2(T) \cong l^2(\mathbb{N})$ by sending $f$ to its Fourier coefficients.
\end{theorem}

This means that the partial Fourier sum converges to $f$ in the $L^2$ norm. 

The Fourier coefficients $\hat{f}_n$ is 
$$
\hat{f}_n = \frac{1}{\sqrt{2 \pi}} \int_T f(x) e^{\inx} dx
$$
We have the Parseval's identity:
$$
\int_T \overline{f(x)} g(x) dx = \sum_{n = -\infty} ^{\infty} \overline{\hat{f}_n} \hat{g}_n
$$
In particular, we can find the $L^2$ norm $f$ by its Fourier coefficients:
$$
\int_T |f(x)|^2 dx = \sum_{n = -\infty} ^{\infty} |\hat{f}_n|^2
$$

We have the convolution (which is really just the group algebra multiplication on $L^2(T)$ induced by the group structure on $T$):
$$
f * g(x) \coloneqq \int_T f(x-y)\ g(y)\  dy
$$

Convolution of $L^2$ functions is $L^2$. And we can calculate its Fourier coefficients:

Fourier transform maps convolution of two functions to the pointwise product. A classic part of abelian duality:
\begin{theorem}
$$
\overline{f * g}_n = \sqrt{2\pi} \hat{f}_n \hat{g}_n
$$
\end{theorem}

We can also ask for other part of convergence. for $L^2$, it converges pointwise a.e., and for continuous differentiable, the convergence is uniform.

The behavior of the partial sums near a point of discontinuity is interesting, not converge uniformly, it oscillate in an interval contains the point of discontinuity. Width of interval shrinks to zero as $N \rightarrow \infty$, but the size of the oscillations does not, the magnitude is approximately $9$ the jump in $f$ at the jump discontinuity.

We have real-valued orthogonal basis 

$$
1, cos\ nx, sin\ nx
$$
This is good for real-valued function as they have real Fourier coefficients. If $f$ is even, then it has a cosine expansion, and $f$ is odd then it has a Fourier sine expansion.

We can also extend to general toruses. Then the theory says that $L^2(T)$ is iso to $l^2(\hat{\Gamma})$, where $\hat{\Gamma}$ is the dual of the lattice $\Gamma$ associated to $T$.

\subsubsection{Smoothness and decay of Fourier coefficients}

The smoother a function is (the more times it is differentiable), the faster its Fourier coefficients decay. That is, smooth function contains a small amount of high frequency components.

If $f \in C^1(T)$ is continuously differentiable, then we can calculate the coefficients of $f'$:
$$
\hat{f'}_n = in\hat{f}_n
$$
Generally $\hat{f^{(k)}}_n= (in)^k \hat{f}_n$

We can use this to define the notion of a function whose derivative is square integrable, not continuous. They are called weak derivative. The space of $L^2$ function whose weak derivative is $L^2$ is called $H^1$, a Sobolev space:

\begin{definition}
The Sobolev space $H^1(T)$ is functions $f \in L^2(T)$ such that 
$$
\sum_{n = -\infty} ^{\infty} n^2 |\hat{f}_n|^2 < \infty
$$
The weak $L^2$ derivative $f' \in L^2(T)$ for $f \in H^1(T)$ is the $L^2$-convergent Fourier series
$$
f'(x) = \frac{1}{\sqrt{2\pi}} \sum_{n = -\infty} ^{\infty} n^2 in \hat{f}_n e^{inx}
$$
\end{definition}

It is a Hilbert space with respect to the inner product 
$$
(f, g)_{H^1} = \int_T \overline{f(x)}g(x) + \overline{f'(x)}g'(x) dx
$$
By Parseval's theorem, the inner product is 
$$
(f,g)_{H^1} = \sum_{n = -\infty} ^{\infty} (1 + n^2)\overline{\hat{f}_n} \hat{g}^n
$$

We can also define weak derivative and $H^1(T)$ by integrating against a smooth test function:

Let $f \in L^2(T)$ and the linear functional $F(\phi): C^1(T) \rightarrow \mathbb{C}$
$$
F(\phi) = - \int_T f\phi' dx
$$
is bounded. Then it uniquely extends to a bounded linear functional on $L^2(T)$, and by Riesz representation theorem, we have a unique function $f' \in L^2(T)$ such that $F(\phi) = (\overline{f}', \phi)$ for all $\phi \in C^1(T)$. This can be taken as our definition of a weak $L^2$ derivative.

For any $k \geq 0$, we define the Sobolev space
$$
H^k(T) = \{f \in L^2(T) | f(x) = \sum_{n = -\infty} ^{\infty} c_n e^{inx}, \sum_{n = -\infty} ^{\infty} |n|^{2k}|c_n|^2 \leq \infty \}
$$

Note this even make sense when $k$ is not a natural number.

Here's a version of Sobolev embedding theorem, which means that if a function on $T$ has a square-integrable weak derivative, then it is continuous. Ofc the only way to show continuity is by showing that the partial Fourier sums (which are ofc continuous) converges uniformly.

\begin{theorem}[Soboloev embedding]
If $f \in H^k(T)$ for $k > 1/2$, then $f \in C(T)$.
\end{theorem}

Generally speaking, if $f \in H^k(T)$, then the Fourier series for $f^{(j)}$ converge uniformly when $k + j + 1/2$, so $f \in C^l(T)$, where $l$ is the greatest integer strictly less than $k - 1/2$. For function of several variable, $f$ is continuous when $k > d/2$ and $j$-times continuous differentiable when $k > j + d/2$. THere is a loss of slight more than one-half a derivative per space dimension in passing from $L^2$ derivatives to continuous derivatives.

\subsubsection{Convergence}
Does the Fourier series of $f$ converges to $f$ is an important question. Here are some results:

A function is piece-wise continuous if it is continuous away from finitely many points and has jump discontinuity at those finite points.

A function is piece-wise smooth if it and its derivative are piece-wise continuous.
\begin{theorem}
If $f(x)$ is piecewise smooth, then the fourier series converges pointwise on the continuous points, and is $\frac{f^-(x) + f^+(x)}{2}$ on jump discountinuity points.
\end{theorem}

\begin{theorem}
If $f(x)$ is continuous and piecewise smooth, then the Fourier series converges uniformly.
\end{theorem}

Of course, for any $L^2$ integrable function $f$, the Fourier series $L^2$ converges to $f$.

\subsection{Fourier Transform}

\subsubsection{FT on Schwartz space and Distributions}
We define Fourier transform of a Schwartz function, it is continuous, one-to-one mapping from $S$ to $S$. Thus we get a continuous, one-to-one on $S^*$.

\begin{definition}
Given $\phi$, we have the Fourier transform $\hat{\phi}$:
$$
\hat{\phi}(k) = \frac{1}{(2\pi)^{n/2}}\int \phi(x)e^{-i k x} dx
$$
The operator is $\mathcal{F}$.
\end{definition}

It takes derivatives to pointwise multiple by its value, and wise versa. It takes translation by rotation with that frequency and vice versa. It takes convolution to pointwise multiplication.

We have the Fourier inverse transform $\mathcal{F}^*$:
$$
\check{\phi}(k) = \frac{1}{(2\pi)^{n/2}}\int \phi(x)e^{i k x} dx
$$

\subsubsection{Fourier transform of tempered distribution}

We can define Fourier transform on tempered distribution as $\hat{T} = \mathcal{F} T$ :
$$
<\hat{T}, \phi> = <T, \hat{\phi}>
$$
Note that for regular distribution this agrees with the Fourier transform definition.
It is an isomorphism. 

The Fourier transform of delta function is a constant.

\subsection{Fourier transform of $L^1$ function}

The Fourier transform of $f$ is well-defined if $f \in L^1$. 
We have the Riemann-Lebesgue lemma:
\begin{theorem}[Riemann-Lebesgue]
For $f \in L^1$, then $\hat{f} \in C_0(\mathbb{R}^n)$, and 
$$
(2\pi)^{n/2} \norm{\hat{f}}_\infty \leq \norm{f}_1
$$
\end{theorem}

Thus Fourier transform is a bounded linear map $L^1 \rightarrow C_0$. $L^1$ with convolution and $C_0$ with pointwise, then Fourier transform maps convolution to pointwise (up to a factor). 

\subsubsection{Fourier Transform on $L^2$}
Fourier transform also gives isomorphism on $L^2(\mathbb{R}^n)$.  We need to define Fourier Transform on $L^2$ function by extend from the dense subspace $S$.

Note that Fourier transform preserves in inner prouduct, thus we extend to an isometric extension $\mathcal{F}: L^2 \rightarrow L^2$.

Thus we have the following:

\begin{theorem}
The Fourier transform $\mathcal{F}$ is a unitary map. 
Thus $(\hat{f}, \hat{g}) = (f, g)$ and 
$$
\int |f(x)|^2 dx = \int |\hat{f}(k)|^2 dk
$$
\end{theorem}

It is a unitary operator so its spectrum on the unit circle. It is entirely consist of eigenvalues.

We can use the Fourier transform to define Sobolev spaces of functions with square-integrable derivatives. 

\begin{definition}
Let $s \in \mathbb{R}$. The Sobolev space $H^s(\mathbb{R}^n)$ consists of all distributions $f \in S^*$ whose Fourier transform $\hat{f}$ is a regular distribution and 
$$
\int (1 + |k|^2)^s |\hat{f}(k)|^2 dk < \infty
$$
\end{definition}

We have that for $f \in H^s(\mathbb{R}^n)$ with $s > n/2$, then $f \in C_0(\mathbb{R}^n)$.
%%%%%%%
\subsubsection{Facts}
For periodic functions we have Fourier series, for non-periodic functions we need all frequency, thus the Fourier transform gives a function on the frequency space.

If $f$ is a compactly supported continuous function, then $\hat{f}$ can be extended to an entire function, that is, a holomorphic function on the entire $\mathbb{C}$ plane.

Here's a useful fact: both $f$ and $\hat{f}$ can't both be compactly supported:

\begin{theorem}
If a continuous $f \in \mathbb{R}$ has $f$ and $\hat{f}$ compactly supported, then $f = 0$
\end{theorem}

\begin{proof}
Here's are three proofs:

1. Note that the Fourier tranform is holomorphic as we can extend to the complex plane in $s$
$$
\hat{f}(s) \coloneqq \frac{1}{\sqrt{2\pi}} \int f(x) e^{isx} dx
$$
as $f$ is compactly supported. Thus $\hat{f}$ can't have too many zero, definitely not compactly supported.

2. We restricts $f$ to an interval containing its support, then in the interval $f$ is equal to the Fourier series. If $\hat{f}$ is compactly supported, this means that $f$ is a finite sum of trigonometric functions, thus analytic, and can't be compactly supported if not zero.

3. We will show that $f$ is analytic. 
\end{proof}



\subsection{General Measure Theory}

\subsubsection{$\sigma$ Algebras, Measureable Spaces, and Measures}
The story of measure theory is about trying to generalize the notion of volume or size. 

We first start with the notion of outer measure:

\begin{definition}
An outer measure $\mu_^*$ on a set $X$ is a function 

$$
\mu^* : 2^X \rightarrow [0, \infty]
$$ 
such that 
\begin{enumerate}
    \item $\mu^*(\emptyset) = 0$;
    \item if $E \subset F \subset X$, then $\mu^*(E) \leq \mu^*(F)$;
    \item (Countable subadditivity) if $E_i$ are countable collection of subsets of $X$, then 
    $$
    \mu^*(\bigcup_{i + 1} ^{\infty} E_i)\leq \sum_{i = 1} ^{\infty} \mu^*(E_i)
    $$
\end{enumerate}
\end{definition}
Note that $\mu^*$ is not assumed to be additive even if ${E_i}$ are disjoint. In fact, they often are not.

The condition we really want is countable additivity, which is that when $E_i$ are disjoint subset, then the it is additive rather than subadditive (aka it is an equal sign). However, this can't always happen (not true over the real line $\mathbb{R}$). So we instead restrict on the subset that we are looking at. This is where we need $\sigma$-algebras:

\begin{definition}
A $\sigma$-algebra on a set $X$ is a connection $\mathcal{A}$ of subsets of $X$ such that
\begin{enumerate}
    \item $\empty, X \in A$;
    \item if $A \in \mathcal{A}$, then so is $A^c \in \mathcal{A}$. $A^c$ is the complement.
    \item if $A_i \in \mathcal{A}$ for $i \in \mathbb{N}$, then 
    $$
    \bigcup_{i=1} ^{\infty} A_i,  \ \bigcap_{i=1} ^{\infty} A_i
    $$
    are in $\mathcal{A}$
\end{enumerate}
\end{definition}
Those is is a collection of subsets that is contains the empty set, close under taking complements and countable unions.
Examples are $\{ \emptyset, X \}$, and $\mathcal{P}(X) = 2^X$.

Now we can define a measureable space, that is, a setting where we can define measures:

\begin{definition}
A measureable space is a pair $(X, \mathcal{A})$, where $X$ is a set and $\mathcal{A}$ is a $\sigma$-algebra on $X$.
\end{definition}

Any topological space gives a measureable space:

\begin{definition}
Given a topolgical space $X$ with topology $\tau$, then the Borel topology is the smallest measureable space containing all the open sets (thus also close sets) of $X$. 
\end{definition}

Given a measureable space $(X, \mathcal{A})$, we can now define a measure:

\begin{definition}
A measure $\mu$ on $(X, \mathcal{A})$ is a function 
$$
\mu: \mathcal{A} \rightarrow [0, \infty]
$$
such that 
\begin{enumerate}
    \item $\mu(\emptyset) = 0$;
    \item It is countably additive.
\end{enumerate}
\end{definition}

Ofc measures restricts to a measureable subset. 

\begin{example}
Let $X$ be a set, then $\nu: \mathcal{P}(X) \rightarrow [0, \infty]$ by cardinality is a measure, called the counting measure. 
\end{example}

A measure zero set, is a measureable net $N$ such that $\mu(N) = 0$. A property hold for all element away from a measure zero set is called to hold almost everywhere, or a.e..

\begin{definition}
A measures space $(X, \mathcal{A}, \mu)$ is complete if every subset of a set of measure zero is measureable.
\end{definition}

There is a completion which takes a measure space to a complete measureable space.

\subsubsection{Measure Functions}
A measureable functions is similiar to a continuous functions. A continuous functions pull back open sets to open setes, a measureable functions pulls back measureable sets to measureable sets.


The once we care about are the real-valued functions. We equipped the target $\mathbb{R}$ with the Borel $\sigma$-algebra. 

\begin{definition}
Let $(X, \mathcal{A})$ be a measureable space, then $f : X \rightarrow \mathbb{R}$ if it pulls back Borel sets to to measureable subsets.
\end{definition}

As the Borel $\sigma$-algebra is generated by $(-\infty, b)$, $(-\infty, b]$, $(a, \infty)$, $[a, \infty)$, the pull back of those sets being measureable is suffice for a function to be measureable.

Of course, products, sums, and inverse of a measureable function is measureable. So is $|f|$, $max(f,g)$, $min(f, g)$.

\todo[inline]{ Finish chapter 3 in this subsubsection}



\begin{lemma}

Pointwise limits ($sup, inf, lim sup, lim inf$) of measurable (extended) functions are measureable (extended) real-valued functions. 
\end{lemma}

Therefore if a sequence of measureable functions are pointwise convergent, then its pointwise limit is measureable (as it is lim sup = lim inf).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Measureable functions are limits of simple functions:
A characteristic function of $E \subset X$ is a function $\chi_E: X \rightarrow \mathbb{R}$ defined as $\chi_E(x) = 1$ iff $x \in E$ and else $0$. It is measureable iff $E$ is a measureable set.

\begin{definition}
A simple function $\phi: X \rightarrow \mathbb{R}$ is a finite $\mathbb{R}$ linear combination of measureable characteristic functions.
\end{definition}


\begin{theorem}
Any positive measurable function is the pointwise limit of a monotone increasing positive simple functions. 
\end{theorem}

\begin{remark}
In Lebesgue integral we approximate by simple functions , in Riemann integral we partition the domain. So Lebesgue integral we partition the range, in Riemann integral we partition the domain.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%
In a complete measure space, then all the proposition above we only need almost every property, such as if $f_n \rightarrow f$ almost everywhere (away from a measure-zero set), then $f$ is measureable.


\subsubsection{Integration}

We develope the general theory of integration of a real-valued function on an measure space. Since every measureable function is the pointwise limit of simple functions, we define the simple ones first and extend from there:

For a simple function $\phi = \sum_{i = 1} ^N c_i \chi_{E_i}$, the integral with respect to measure $\mu$ is 
$$
\int \phi d\mu = \sum_{i = 1} ^N c_i \mu(E_i).
$$

\begin{example}
The characteristic function of the rations $\chi_\mathbb{Q}: \mathbb{R} \rightarrow \mathbb{R}$ is not Riemann integrable, but they are Lebesgue integrable with $\int \chi_\mathbb{Q} d\mu = 0$.
\end{example}

For a positive function $f$, then 
$$
\int f d\mu = sup{\int \phi d\mu: 0 \leq \phi \leq f, \phi \ simple}
$$
It is integrable if it is measureable and $\int f d\mu < \infty$


Let $A$ be a measureable set, then 
$$
\int_A f d\mu = \int f_{\chi_A}d\mu
$$

Here's the important Monotone Convergence Theorem:

\begin{theorem}[Monotone Convergence Theorem]
If $\{f_n\}$ is a monotone increasing sequence of positive, measureable, extended real-valued functions, and 
$$
f = lim_{n\rightarrow \infty} f_n,
$$
then 
$$
lim_{n \rightarrow \infty} \int f_n d\mu = \int f d\mu
$$
\end{theorem}

This means that $f$ is the limit of integrals of an increasing sequence of simple functions. This means the following:

\begin{lemma}
In the $L^1$ norm, the space of simple functions are dense. That is, any measureable function is a limit of simple functions in the $L^1$ norm.
\end{lemma}

The measure of an extended real-valued function is its positive part  + the negative part.
It is integrable iff $\int |f| d\mu < \infty$. Note that absolutely convergence of a series is a version of integrable functions.

An important question of integration theory is given $f_n$, does $$
lim_{n \rightarrow \infty} f_n d \mu = \int lim_{n \rightarrow \infty} f_n d \mu
$$.
It is definitely not true in general. 

Here's two important theorems about it:

\begin{theorem}[Fatou's lemma]
Let $f_n$ be sequence of positive measureable functions, then 
$$
\int lim\ inf f_n d\mu \leq lim in \int f_n d\mu
$$
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We also have the dominated convergence theorem:
\begin{theorem}[Dominated Convergence Theorem]
if $f_n$ is a sequence of measureable functions $f_n: X \rightarrow \mathbb{R}$ such that $f_n \rightarrow f$ pointwise, and $|f_n| \leq g$ for $g$ integrable, then
$$
\int f_n d\mu \rightarrow \int f d\mu 
$$
\end{theorem}

Of course, we can generalize all of the above to complex-valued functions.

\begin{theorem}
Of course any Riemann integrable function is Lebesgue integrable and its Riemann integral is equal to Lebesgue intergral. Moreoever, a Lebesgue measureable function is Riemann integrable iff it is bounded and the set of discontinuities has Lebesgue measure zero.
\end{theorem}

\subsubsection{$L^p$ spaces}

\begin{definition}
Given a measure space $X, \mathcal{A}, \mu$ and $1 \leq p < \infty$, the space $L^p(X)$ is equivalence classes of measureable functions $f: X \rightarrow \mathbb{R}$. such that
$$
\int |f|^p d\mu < \infty
$$, 
where two measureable functions are equivalent if they are equal $\mu$-a.e... The $L^p$-norm of $f$ is 
$$
\norm{f}_{L_p} = (\int |f|^p d\mu )^{1/p}
$$

\end{definition}
We also have the $L^\infty$ norm:
\begin{definition}
Let $f$ be a measureable function. The essential supremum is 
$$
ess sup f = inf \{a \in \mathbb{R}: \mu \{x \in X : f(x) > a \} = 0 \}
$$

It is essentailly bounded if $ess sup|f| < \infty$

The space of $L^\infty(X)$ consists of point-wise a.e.-equivalence classes of essentially bounded function with norm $ess sup|f|$.
\end{definition}

Here's are two important inequalies:

\begin{theorem}[Minkowski inequility]
If $f, g \in L^p(X)$ for $1 \leq p \leq \infty$, then $f + g \in L^p(X)$ and 
$$
\norm{f + g}_{L^p} \leq \norm{f}_{L^p} + \norm{g}_{L^p}
$$
\end{theorem}

\begin{definition}
For $p$ with $1 \leq p \leq \infty$, 
The Holder conjuage of $p$ if $p'$ with 
$$
\frac{1}{p} + \frac{1}{p'} = 1
$$
$1' = \infty$ and $\infty' = 1$.
\end{definition}

\begin{theorem}[Holder's inequality]
If $f \in L^p(X)$ and $g \in L^{p'}(X)$, then $fg \in L^1(X)$ and 
$$
\int |fg| d\mu \leq \norm{f}_{L^p}\norm{g}_{L^{p'}}
$$
\end{theorem}

Here's a immediate generalization, which is even better:

\begin{theorem}[Holder's Inequality]
Given $r, p, q$ with $\frac{1}{r} = \frac{1}{p} + \frac{1}{q}$, then 
$$
\norm{fg}_r \leq \norm{f}_{p}  \norm{f}_q 
$$

\end{theorem}

Simple functions are dense in $L^p$ for all $p$:

\begin{theorem}
The simple functions that belong to $L^p(X)$ are dense in $L^p(X)$.
\end{theorem}

Note that a simple function $\phi = \sum_{i=1} ^n c_i \chi_{A_i}$ belong in $L^p$ for $p < \infty$ iff $\mu(A_i) < \infty$. Every simple function belong in $L^\infty$.

In addition, $L^p(X)$ are also complete:

\begin{theorem}[Riesz-Fischer Theorem]
If $X$ is a measure space and $1 \leq p \leq \infty$, then $L^p(X)$ is complete.
\end{theorem}

Note that for $1 \leq p < \infty$, any converging sequence has a pointwise converging subsequence.

%%%%%%%%%%%%%%%%%%%%%
We also have duality for $L^p$ spaces:
The dual $X^*$ of a Banach space $X$, as a set, is all bounded linear functionals on $X$.

\begin{theorem}[Duality of $L^p$ spaces]
Given $1 < p \leq \infty$, if $f \in L^{p'}(X)$, then 
$$
F(g) = \int fg d\mu 
$$
defines a bounded linear functional $F: L^p(X) \rightarrow \mathbb{R}$ with 
$$
\norm{F} = \norm{f}_{L^{p'}}
$$.
For $1 < p < \infty$, this defines an isomorphism $L^{p'}(X) \cong L^p(X)^*$
\end{theorem}

\begin{corollary}
For those $p$, $L^p(X)$ is reflexive, that is, the canonical map $L^p(X) \rightarrow L^p(X)^{**}$ is an isomorphism.
\end{corollary}

For $X$ a finite measure space, for $p < q$, we have that $L^q(X) \xrightarrow{} L^p(X)$:

\begin{theorem}
For $X$ a finite measure space, for $1 \leq p \leq q \leq \infty$, then the $L^q$ norm bounds $L^p$ norm. Thus we have inclusion $L^q(X) \xrightarrow{} L^p(X)$. 
\end{theorem}

Thus we have uniform convergence implies $L^\infty$ implies $L^p$ ... implies $L^1$.

\subsubsection{Convergences}
On a measure space, there is a notion of convergence in measure:

\begin{definition}
A sequence $f_n \rightarrow f$ converges in measure if for any $\epsilon > 0$, then the limit 
$$
\mu(\{x \in X | |f(x) - f_n(x)| > \epsilon\} \rightarrow 0
$$

\end{definition}
So in a measure space, we have uniform convergence, $L^p$ convergences for $1 \leq p \leq \infty$, pointwise convergence a.e. and convergence in measure.

In general, $L^p$ implies convergence in measure. In a finite measure space, pointwise convergence implies convergence in measure. And convergence in measure implies a subsequence that is pointwise convergent.
\subsection{Lebesgue measure on $\mathbb{R}^n$}

The Lebesgue measure is constructed from elementary geometrical object like cubes. First we describe the Lebesgue $\sigma$-algebra.

\begin{theorem}
The Lebesgue $\sigma$-algebra is simply the completion of the Borel $\sigma$-algebra on $\mathbb{R}^n$.
\end{theorem}

Here's are the measure zero sets:
\begin{lemma}
A subset $N \subset \mathbb{R}^n$ has Lebesgue measure zero if for every $\epsilon > 0$, there exists a countable collection of rectanges $R_i$ such that $N \subset \bigcup_{i = 0}^\infty R_i$, and $\sum_{i = 1} ^\infty \mu(R_i) < \epsilon$.
\end{lemma}

Every countable set has measure zero, the Cantor set is an uncountable measure zero set.

The Lebesgue measure is translational invariant, it is the Haar measure. It is also translational invariant. 


%%%%%%%%%%%%%%%%
\subsubsection{Regularity Results}
We also have regularity results for the Lebesgue measure. 

\begin{theorem}
If $A \subset \mathbb{R}^n$, then 
$$
\mu^*(A) = inf\{\mu(G): A \subset G, G open\},
$$
and if $A$ is Lebesgue measurable, then 
$$
\mu(A) = sup \{\mu(K): K \subset A, K compact\}.
$$
\end{theorem}

A subset $A$ is Lebesgue measureable iff there are open sets containing $A$ and difference is arbitrary small.

Here's another characterization, which says the Lebesgue measureable sets are the ones that can be "squeezed" between open and closed sets:

\begin{theorem}
A subset $A$ is Lebesgue measurable iff if for every $\epsilon > 0$ there is an open set $G$ and closed set $F$ s.y. $F \subset A \subset G$ and $\mu(G\\F)< \epsilon.$ If $\mu(A) < \infty$, then $F$ can be chosen to be compact.
\end{theorem}

Thus we can approximate a Lebesgue measureable set by an open containing it and a closed contained in it, with arbitrary small, but generally nonzero difference.


\subsubsection{$L^p$ functions on $\mathbb{R}^n$}
\begin{theorem}
For $1 \leq p < \infty$, we have that the space of compactly supported functions is dense in $L^p(\mathbb{R}^n)$.
\end{theorem}

Let $\mu$ be the Lebesgue Measure on the Real number line $\mathbb{R}$. We want to study how integration on the real number work:

\begin{definition}
Let $f$ be a non-negative function on $\mathbb{R}$, it is Lebesgue measureable if 
\end{definition}



\subsection{Probability}
In measure theory point-of-view, probability theory is just the study of measure theory on a particular nice measure space, namely, one that's total measure 1. However, they are their own languages, which I will try to describe here.

\begin{defintion}

A probability space $\Omega$ is a finite measure space with total measure 1. A random variable $X$ is a real-valued measureable function.
\end{defintion}


\begin{definition}
The mean/expectation of $X$, $\mathbb{E}[X]$ is simply the Lebesgue integral 
$$
\int X d\mu
$$
More generally, given a function $\phi: \mathbb{R} \rightarrow \mathbb{R}$, we have $\mathbb{E}[\phi] \coloneqq \int \phi(X) d\mu$. 
\end{definition}

We also use $\mathbb{P}$:
\begin{definition}
$$
\mathbb{P}[X < a] \coloneqq \mu (X^{-1}(-\infty, a))
$$
\end{definition}
\begin{definition}
The variance of $X$ is $var(X) \coloneqq \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$
\end{definition}

Given a random variable $X$, we have its cumulative distribution function $F(X,x)$:

\begin{definition}
The cumulative distribution function (CDF) $F_X(x)$ is a real-valued function on $\mathbb{R}$: 

$$
F_X(x) = \mathbb{P}[X \leq x]
$$

\end{definition}

The derivative of this in the density function, it is the Radon-Nikodym derivative $f = \frac{d X_*\mu}{d \mu_{L}}$, where $\mu_{L}$ is the Lebesgue measure on $\mathbb{R}$. It has the property that
$$
\int_a ^b f dx = \mathbb{P}[a < X < b]
$$

Note that 
$$
\mathbb{E}[\phi] = \int_\Omega \phi(X) d\mu = \int_\mathbb{R} \phi d X_*(\mu) = \int_\mathbb{R} \phi f dx
$$

\begin{example}
The most important distribution is of course the Gaussian (normal) distribution $N(\mu, \sigma)$, where $\mu$ is the mean and $\sigma$ is the standard deviation. Its density function is 
$$
\frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} (\frac{x-\mu}{\sigma} )^2}
$$

\end{example}


\begin{definition}

Two random variables are independent iff $F_{X,Y}(x,y) = F_X(x) F_Y(y)$, where $F_{X,Y}(x,y)$ is the probability that $X < x$ and $Y < y$. 

$n$ random variables are pairwise independent if any two are independent. They are mutually independent iff $F_{X_1, ..}(x_1, ..) = F_{X_1}(x_1) ...$.
\end{definition}

Equivalently, $X_1, ... , X_n$ are independent iff the pushforward measure $(X_1, ..., X_n)_* \mu$ of the map 
$(X_1, ... X_n): \Omega \rightarrow \mathbb{R}^n$ is the product measure of $X_1_* \mu ... $

Thus by Fubini's theorem, we have that
$$
\mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[Y]
$$
thus 
$$
var(XY) = var(X) + var(Y)
$$



We have the central limit theorem, which describe what happens when we look at the partial sum of the same independent experiment done again and again, in the limit of infinity experiments:



\begin{theorem}
Let $X_1, ...$ be i.i.d (independent identical density) random variables with mean $\mu$ and standard deviation $\sigma$. Let $S_n = \frac{X_1 + X_2 + .. + X_n}{\sqrt{n}}$, then $S_n$ tends to the normal distribution $N(\sqrt{\mu}, \sigma) $ with mean $\sqrt{\mu}$ and standard deviation $\sigma$. 
\end{theorem}

\end{document}